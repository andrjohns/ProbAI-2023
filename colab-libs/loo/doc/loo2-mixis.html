<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1" />

<meta name="author" content="Luca Silva and Giacomo Zanella" />

<meta name="date" content="2023-03-30" />

<title>Mixture IS leave-one-out cross-validation for high-dimensional Bayesian models</title>

<script>// Pandoc 2.9 adds attributes on both header and div. We remove the former (to
// be compatible with the behavior of Pandoc < 2.8).
document.addEventListener('DOMContentLoaded', function(e) {
  var hs = document.querySelectorAll("div.section[class*='level'] > :first-child");
  var i, h, a;
  for (i = 0; i < hs.length; i++) {
    h = hs[i];
    if (!/^h[1-6]$/i.test(h.tagName)) continue;  // it should be a header h1-h6
    a = h.attributes;
    while (a.length > 0) h.removeAttribute(a[0].name);
  }
});
</script>

<style type="text/css">
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
span.underline{text-decoration: underline;}
div.column{display: inline-block; vertical-align: top; width: 50%;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
</style>



<style type="text/css">
code {
white-space: pre;
}
.sourceCode {
overflow: visible;
}
</style>
<style type="text/css" data-origin="pandoc">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
{ counter-reset: source-line 0; }
pre.numberSource code > span
{ position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
{ content: counter(source-line);
position: relative; left: -1em; text-align: right; vertical-align: baseline;
border: none; display: inline-block;
-webkit-touch-callout: none; -webkit-user-select: none;
-khtml-user-select: none; -moz-user-select: none;
-ms-user-select: none; user-select: none;
padding: 0 4px; width: 4em;
color: #aaaaaa;
}
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa; padding-left: 4px; }
div.sourceCode
{ }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } 
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.at { color: #7d9029; } 
code span.bn { color: #40a070; } 
code span.bu { color: #008000; } 
code span.cf { color: #007020; font-weight: bold; } 
code span.ch { color: #4070a0; } 
code span.cn { color: #880000; } 
code span.co { color: #60a0b0; font-style: italic; } 
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.do { color: #ba2121; font-style: italic; } 
code span.dt { color: #902000; } 
code span.dv { color: #40a070; } 
code span.er { color: #ff0000; font-weight: bold; } 
code span.ex { } 
code span.fl { color: #40a070; } 
code span.fu { color: #06287e; } 
code span.im { color: #008000; font-weight: bold; } 
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.kw { color: #007020; font-weight: bold; } 
code span.op { color: #666666; } 
code span.ot { color: #007020; } 
code span.pp { color: #bc7a00; } 
code span.sc { color: #4070a0; } 
code span.ss { color: #bb6688; } 
code span.st { color: #4070a0; } 
code span.va { color: #19177c; } 
code span.vs { color: #4070a0; } 
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } 
</style>
<script>
// apply pandoc div.sourceCode style to pre.sourceCode instead
(function() {
  var sheets = document.styleSheets;
  for (var i = 0; i < sheets.length; i++) {
    if (sheets[i].ownerNode.dataset["origin"] !== "pandoc") continue;
    try { var rules = sheets[i].cssRules; } catch (e) { continue; }
    var j = 0;
    while (j < rules.length) {
      var rule = rules[j];
      // check if there is a div.sourceCode rule
      if (rule.type !== rule.STYLE_RULE || rule.selectorText !== "div.sourceCode") {
        j++;
        continue;
      }
      var style = rule.style.cssText;
      // check if color or background-color is set
      if (rule.style.color === '' && rule.style.backgroundColor === '') {
        j++;
        continue;
      }
      // replace div.sourceCode by a pre.sourceCode rule
      sheets[i].deleteRule(j);
      sheets[i].insertRule('pre.sourceCode{' + style + '}', j);
    }
  }
})();
</script>




<style type="text/css">body {
background-color: #fff;
margin: 1em auto;
max-width: 700px;
overflow: visible;
padding-left: 2em;
padding-right: 2em;
font-family: "Open Sans", "Helvetica Neue", Helvetica, Arial, sans-serif;
font-size: 14px;
line-height: 1.35;
}
#TOC {
clear: both;
margin: 0 0 10px 10px;
padding: 4px;
width: 400px;
border: 1px solid #CCCCCC;
border-radius: 5px;
background-color: #f6f6f6;
font-size: 13px;
line-height: 1.3;
}
#TOC .toctitle {
font-weight: bold;
font-size: 15px;
margin-left: 5px;
}
#TOC ul {
padding-left: 40px;
margin-left: -1.5em;
margin-top: 5px;
margin-bottom: 5px;
}
#TOC ul ul {
margin-left: -2em;
}
#TOC li {
line-height: 16px;
}
table {
margin: 1em auto;
border-width: 1px;
border-color: #DDDDDD;
border-style: outset;
border-collapse: collapse;
}
table th {
border-width: 2px;
padding: 5px;
border-style: inset;
}
table td {
border-width: 1px;
border-style: inset;
line-height: 18px;
padding: 5px 5px;
}
table, table th, table td {
border-left-style: none;
border-right-style: none;
}
table thead, table tr.even {
background-color: #f7f7f7;
}
p {
margin: 0.5em 0;
}
blockquote {
background-color: #f6f6f6;
padding: 0.25em 0.75em;
}
hr {
border-style: solid;
border: none;
border-top: 1px solid #777;
margin: 28px 0;
}
dl {
margin-left: 0;
}
dl dd {
margin-bottom: 13px;
margin-left: 13px;
}
dl dt {
font-weight: bold;
}
ul {
margin-top: 0;
}
ul li {
list-style: circle outside;
}
ul ul {
margin-bottom: 0;
}
pre, code {
background-color: #f7f7f7;
border-radius: 3px;
color: #333;
white-space: pre-wrap; 
}
pre {
border-radius: 3px;
margin: 5px 0px 10px 0px;
padding: 10px;
}
pre:not([class]) {
background-color: #f7f7f7;
}
code {
font-family: Consolas, Monaco, 'Courier New', monospace;
font-size: 85%;
}
p > code, li > code {
padding: 2px 0px;
}
div.figure {
text-align: center;
}
img {
background-color: #FFFFFF;
padding: 2px;
border: 1px solid #DDDDDD;
border-radius: 3px;
border: 1px solid #CCCCCC;
margin: 0 5px;
}
h1 {
margin-top: 0;
font-size: 35px;
line-height: 40px;
}
h2 {
border-bottom: 4px solid #f7f7f7;
padding-top: 10px;
padding-bottom: 2px;
font-size: 145%;
}
h3 {
border-bottom: 2px solid #f7f7f7;
padding-top: 10px;
font-size: 120%;
}
h4 {
border-bottom: 1px solid #f7f7f7;
margin-left: 8px;
font-size: 105%;
}
h5, h6 {
border-bottom: 1px solid #ccc;
font-size: 105%;
}
a {
color: #0033dd;
text-decoration: none;
}
a:hover {
color: #6666ff; }
a:visited {
color: #800080; }
a:visited:hover {
color: #BB00BB; }
a[href^="http:"] {
text-decoration: underline; }
a[href^="https:"] {
text-decoration: underline; }

code > span.kw { color: #555; font-weight: bold; } 
code > span.dt { color: #902000; } 
code > span.dv { color: #40a070; } 
code > span.bn { color: #d14; } 
code > span.fl { color: #d14; } 
code > span.ch { color: #d14; } 
code > span.st { color: #d14; } 
code > span.co { color: #888888; font-style: italic; } 
code > span.ot { color: #007020; } 
code > span.al { color: #ff0000; font-weight: bold; } 
code > span.fu { color: #900; font-weight: bold; } 
code > span.er { color: #a61717; background-color: #e3d2d2; } 
</style>




</head>

<body>




<h1 class="title toc-ignore">Mixture IS leave-one-out cross-validation
for high-dimensional Bayesian models</h1>
<h4 class="author">Luca Silva and Giacomo Zanella</h4>
<h4 class="date">2023-03-30</h4>


<div id="TOC">
<ul>
<li><a href="#introduction" id="toc-introduction">Introduction</a>
<ul>
<li><a href="#setup-load-packages-and-set-seed" id="toc-setup-load-packages-and-set-seed">Setup: load packages and set
seed</a></li>
<li><a href="#model" id="toc-model">Model</a></li>
<li><a href="#dataset" id="toc-dataset">Dataset</a></li>
<li><a href="#psis-estimators-and-pareto-k-diagnostics" id="toc-psis-estimators-and-pareto-k-diagnostics">PSIS estimators and
Pareto-<span class="math inline">\(k\)</span> diagnostics</a></li>
<li><a href="#mixture-estimators" id="toc-mixture-estimators">Mixture
estimators</a></li>
<li><a href="#comparison-with-benchmark-values-obtained-with-long-simulations" id="toc-comparison-with-benchmark-values-obtained-with-long-simulations">Comparison
with benchmark values obtained with long simulations</a></li>
<li><a href="#references" id="toc-references">References</a></li>
</ul></li>
</ul>
</div>

<!--
%\VignetteEngine{knitr::rmarkdown}
%\VignetteIndexEntry{Mixture IS leave-one-out cross-validation for high-dimensional Bayesian models}
-->
<div id="introduction" class="section level1">
<h1>Introduction</h1>
<p>This vignette shows how to perform Bayesian leave-one-out
cross-validation (LOO-CV) using the mixture estimators proposed in the
paper <a href="https://arxiv.org/abs/2209.09190">Silva and Zanella
(2022)</a>. These estimators have shown to be useful in presence of
outliers but also, and especially, in high-dimensional settings where
the model features many parameters. In these contexts it can happen that
a large portion of observations lead to high values of Pareto-<span class="math inline">\(k\)</span> diagnostics and potential instability
of PSIS-LOO estimators.</p>
<p>For this illustration we consider a high-dimensional Bayesian
Logistic regression model applied to the <em>Voice</em> dataset.</p>
<div id="setup-load-packages-and-set-seed" class="section level2">
<h2>Setup: load packages and set seed</h2>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">&quot;rstan&quot;</span>)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">&quot;loo&quot;</span>)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">&quot;matrixStats&quot;</span>)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="fu">options</span>(<span class="at">mc.cores =</span> parallel<span class="sc">::</span><span class="fu">detectCores</span>(), <span class="at">parallel=</span><span class="cn">FALSE</span>)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">24877</span>)</span></code></pre></div>
</div>
<div id="model" class="section level2">
<h2>Model</h2>
<p>This is the Stan code for a logistic regression model with
regularized horseshoe prior. The code includes an if statement to
include a code line needed later for the MixIS approach.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>stancode_horseshoe <span class="ot">&lt;-</span> <span class="st">&quot;</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="st">data {</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="st">  int &lt;lower=0&gt; n;                </span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="st">  int &lt;lower=0&gt; p;                </span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="st">  int &lt;lower=0, upper=1&gt; y[n];    </span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="st">  matrix [n,p] X;                </span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="st">  real &lt;lower=0&gt; scale_global;</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="st">  int &lt;lower=0,upper=1&gt; mixis;                </span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="st">}</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="st">transformed data {</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a><span class="st">  real&lt;lower=1&gt; nu_global=1; // degrees of freedom for the half-t priors for tau</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a><span class="st">  real&lt;lower=1&gt; nu_local=1;  // degrees of freedom for the half-t priors for lambdas</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="st">                             // (nu_local = 1 corresponds to the horseshoe)</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="st">  real&lt;lower=0&gt; slab_scale=2;// for the regularized horseshoe</span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a><span class="st">  real&lt;lower=0&gt; slab_df=100; // for the regularized horseshoe</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a><span class="st">}</span></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a><span class="st">parameters {</span></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a><span class="st">  vector[p] z;                // for non-centered parameterization</span></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a><span class="st">  real &lt;lower=0&gt; tau;         // global shrinkage parameter</span></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a><span class="st">  vector &lt;lower=0&gt;[p] lambda; // local shrinkage parameter</span></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a><span class="st">  real&lt;lower=0&gt; caux;</span></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a><span class="st">}</span></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a><span class="st">transformed parameters {</span></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a><span class="st">  vector[p] beta;</span></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a><span class="st">  { </span></span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a><span class="st">    vector[p] lambda_tilde;   // &#39;truncated&#39; local shrinkage parameter</span></span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a><span class="st">    real c = slab_scale * sqrt(caux); // slab scale</span></span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a><span class="st">    lambda_tilde = sqrt( c^2 * square(lambda) ./ (c^2 + tau^2*square(lambda)));</span></span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a><span class="st">    beta = z .* lambda_tilde*tau;</span></span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a><span class="st">  }</span></span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a><span class="st">}</span></span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a><span class="st">model {</span></span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a><span class="st">  vector[n] means=X*beta;</span></span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a><span class="st">  vector[n] log_lik;</span></span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a><span class="st">  target += std_normal_lpdf(z);</span></span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a><span class="st">  target += student_t_lpdf(lambda | nu_local, 0, 1);</span></span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a><span class="st">  target += student_t_lpdf(tau | nu_global, 0, scale_global);</span></span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a><span class="st">  target += inv_gamma_lpdf(caux | 0.5*slab_df, 0.5*slab_df);</span></span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a><span class="st">  for (index in 1:n) {</span></span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a><span class="st">    log_lik[index]= bernoulli_logit_lpmf(y[index] | means[index]);</span></span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a><span class="st">  }</span></span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a><span class="st">  target += sum(log_lik);</span></span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true" tabindex="-1"></a><span class="st">  if (mixis) {</span></span>
<span id="cb2-44"><a href="#cb2-44" aria-hidden="true" tabindex="-1"></a><span class="st">    target += log_sum_exp(-log_lik);</span></span>
<span id="cb2-45"><a href="#cb2-45" aria-hidden="true" tabindex="-1"></a><span class="st">  }</span></span>
<span id="cb2-46"><a href="#cb2-46" aria-hidden="true" tabindex="-1"></a><span class="st">}</span></span>
<span id="cb2-47"><a href="#cb2-47" aria-hidden="true" tabindex="-1"></a><span class="st">generated quantities {</span></span>
<span id="cb2-48"><a href="#cb2-48" aria-hidden="true" tabindex="-1"></a><span class="st">  vector[n] means=X*beta;</span></span>
<span id="cb2-49"><a href="#cb2-49" aria-hidden="true" tabindex="-1"></a><span class="st">  vector[n] log_lik;</span></span>
<span id="cb2-50"><a href="#cb2-50" aria-hidden="true" tabindex="-1"></a><span class="st">  for (index in 1:n) {</span></span>
<span id="cb2-51"><a href="#cb2-51" aria-hidden="true" tabindex="-1"></a><span class="st">    log_lik[index] = bernoulli_logit_lpmf(y[index] | means[index]);</span></span>
<span id="cb2-52"><a href="#cb2-52" aria-hidden="true" tabindex="-1"></a><span class="st">  }</span></span>
<span id="cb2-53"><a href="#cb2-53" aria-hidden="true" tabindex="-1"></a><span class="st">}</span></span>
<span id="cb2-54"><a href="#cb2-54" aria-hidden="true" tabindex="-1"></a><span class="st">&quot;</span></span></code></pre></div>
</div>
<div id="dataset" class="section level2">
<h2>Dataset</h2>
<p>The <em>LSVT Voice Rehabilitation Data Set</em> (see <a href="https://archive.ics.uci.edu/ml/datasets/LSVT+Voice+Rehabilitation">link</a>
for details) has <span class="math inline">\(p=312\)</span> covariates
and <span class="math inline">\(n=126\)</span> observations with binary
response. We construct data list for Stan.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(voice)</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> voice<span class="sc">$</span>y</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> voice[<span class="dv">2</span><span class="sc">:</span><span class="fu">length</span>(voice)]</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="fu">dim</span>(X)[<span class="dv">1</span>]</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="fu">dim</span>(X)[<span class="dv">2</span>]</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>p0 <span class="ot">&lt;-</span> <span class="dv">10</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>scale_global <span class="ot">&lt;-</span> <span class="dv">2</span><span class="sc">*</span>p0<span class="sc">/</span>(p<span class="sc">-</span>p0)<span class="sc">/</span><span class="fu">sqrt</span>(n<span class="dv">-1</span>)</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>standata <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="at">n =</span> n, <span class="at">p =</span> p, <span class="at">X =</span> <span class="fu">as.matrix</span>(X), <span class="at">y =</span> <span class="fu">c</span>(y), <span class="at">scale_global =</span> scale_global, <span class="at">mixis =</span> <span class="dv">0</span>)</span></code></pre></div>
<p>Note that in our prior specification we divide the prior variance by
the number of covariates <span class="math inline">\(p\)</span>. This is
often done in high-dimensional contexts to have a prior variance for the
linear predictors <span class="math inline">\(X\beta\)</span> that
remains bounded as <span class="math inline">\(p\)</span> increases.</p>
</div>
<div id="psis-estimators-and-pareto-k-diagnostics" class="section level2">
<h2>PSIS estimators and Pareto-<span class="math inline">\(k\)</span>
diagnostics</h2>
<p>LOO-CV computations are challenging in this context due to
high-dimensionality of the parameter space. To show that, we compute
PSIS-LOO estimators, which require sampling from the posterior
distribution, and inspect the associated Pareto-<span class="math inline">\(k\)</span> diagnostics.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>chains <span class="ot">&lt;-</span> <span class="dv">4</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>n_iter <span class="ot">&lt;-</span> <span class="dv">2000</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>warm_iter <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>stanmodel <span class="ot">&lt;-</span> <span class="fu">stan_model</span>(<span class="at">model_code =</span> stancode_horseshoe)</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>fit_post <span class="ot">&lt;-</span> <span class="fu">sampling</span>(stanmodel, <span class="at">data =</span> standata, <span class="at">chains =</span> chains, <span class="at">iter =</span> n_iter, <span class="at">warmup =</span> warm_iter, <span class="at">refresh =</span> <span class="dv">0</span>)</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>loo_post <span class="ot">&lt;-</span><span class="fu">loo</span>(fit_post)</span></code></pre></div>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(loo_post)</span></code></pre></div>
<pre><code>
Computed from 4000 by 126 log-likelihood matrix

         Estimate   SE
elpd_loo    -42.5  7.1
p_loo        23.8  5.2
looic        85.0 14.2
------
Monte Carlo SE of elpd_loo is NA.

Pareto k diagnostic values:
                         Count Pct.    Min. n_eff
(-Inf, 0.5]   (good)     29    23.0%   386       
 (0.5, 0.7]   (ok)       68    54.0%   82        
   (0.7, 1]   (bad)      23    18.3%   13        
   (1, Inf)   (very bad)  6     4.8%   4         
See help(&#39;pareto-k-diagnostic&#39;) for details.</code></pre>
<p>As we can see the diagnostics signal either “bad” or “very bad”
Pareto-<span class="math inline">\(k\)</span> values for roughly <span class="math inline">\(15-30\%\)</span> of the observations which is a
significant portion of the dataset.</p>
</div>
<div id="mixture-estimators" class="section level2">
<h2>Mixture estimators</h2>
<p>We now compute the mixture estimators proposed in Silva and Zanella
(2022). These require to sample from the following mixture of
leave-one-out posteriors <span class="math display">\[\begin{equation}
q_{mix}(\theta) =  \frac{\sum_{i=1}^n
p(y_{-i}|\theta)p(\theta)}{\sum_{i=1}^np(y_{-i})}\propto
p(\theta|y)\cdot \left(\sum_{i=1}^np(y_i|\theta)^{-1}\right).
\end{equation}\]</span> The code to generate a Stan model for the above
mixture distribution is the same to the one for the posterior, just
enabling one line of code with a <em>LogSumExp</em> contribution to
account for the last term in the equation above.</p>
<pre><code>  if (mixis) {
    target += log_sum_exp(-log_lik);
  }</code></pre>
<p>We sample from the mixture and collect the log-likelihoods term.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>standata<span class="sc">$</span>mixis <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>fit_mix <span class="ot">&lt;-</span> <span class="fu">sampling</span>(stanmodel, <span class="at">data =</span> standata, <span class="at">chains =</span> chains, <span class="at">iter =</span> n_iter, <span class="at">warmup =</span> warm_iter, <span class="at">refresh =</span> <span class="dv">0</span>, <span class="at">pars =</span> <span class="st">&quot;log_lik&quot;</span>)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>log_lik_mix <span class="ot">&lt;-</span> <span class="fu">extract</span>(fit_mix)<span class="sc">$</span>log_lik</span></code></pre></div>
<p>We now compute the mixture estimators, following the numerically
stable implementation in Appendix A.2 of <a href="https://arxiv.org/abs/2209.09190">Silva and Zanella (2022)</a>.
The code below makes use of the package “matrixStats”.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>l_common_mix <span class="ot">&lt;-</span> <span class="fu">rowLogSumExps</span>(<span class="sc">-</span>log_lik_mix)</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>log_weights <span class="ot">&lt;-</span> <span class="sc">-</span>log_lik_mix <span class="sc">-</span> l_common_mix</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>elpd_mixis <span class="ot">&lt;-</span> <span class="fu">logSumExp</span>(<span class="sc">-</span>l_common_mix) <span class="sc">-</span> <span class="fu">rowLogSumExps</span>(<span class="fu">t</span>(log_weights))</span></code></pre></div>
</div>
<div id="comparison-with-benchmark-values-obtained-with-long-simulations" class="section level2">
<h2>Comparison with benchmark values obtained with long simulations</h2>
<p>To evaluate the performance of mixture estimators (MixIS) we also
generate <em>benchmark values</em>, i.e. accurate approximations of the
LOO predictives <span class="math inline">\(\{p(y_i|y_{-i})\}_{i=1,\dots,n}\)</span>, obtained
by brute-force sampling from the leave-one-out posteriors directly,
getting <span class="math inline">\(90k\)</span> samples from each and
discarding the first <span class="math inline">\(10k\)</span> as warmup.
This is computationally heavy, hence we have saved the results and we
just load them in the current vignette.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(voice_loo)</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>elpd_loo <span class="ot">&lt;-</span> voice_loo<span class="sc">$</span>elpd_loo</span></code></pre></div>
<p>We can then compute the root mean squared error (RMSE) of the PSIS
and mixture estimators relative to such benchmark values.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>elpd_psis <span class="ot">&lt;-</span> loo_post<span class="sc">$</span>pointwise[,<span class="dv">1</span>]</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">paste</span>(<span class="st">&quot;RMSE(PSIS) =&quot;</span>,<span class="fu">round</span>( <span class="fu">sqrt</span>(<span class="fu">mean</span>((elpd_loo<span class="sc">-</span>elpd_psis)<span class="sc">^</span><span class="dv">2</span>)) ,<span class="dv">2</span>)))</span></code></pre></div>
<pre><code>[1] &quot;RMSE(PSIS) = 0.08&quot;</code></pre>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">paste</span>(<span class="st">&quot;RMSE(MixIS) =&quot;</span>,<span class="fu">round</span>( <span class="fu">sqrt</span>(<span class="fu">mean</span>((elpd_loo<span class="sc">-</span>elpd_mixis)<span class="sc">^</span><span class="dv">2</span>)) ,<span class="dv">2</span>)))</span></code></pre></div>
<pre><code>[1] &quot;RMSE(MixIS) = 0.05&quot;</code></pre>
<p>Here mixture estimator provides a reduction in RMSE. Note that this
value would increase with the number of samples drawn from the posterior
and mixture, since in this example the RMSE of MixIS will exhibit a
CLT-type decay while the one of PSIS will converge at a slower rate
(this can be verified by running the above code with a larger sample
size; see also Figure 3 of Silva and Zanella (2022) for analogous
results).</p>
<p>We then compare the overall ELPD estimates with the brute force
one.</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>elpd_psis <span class="ot">&lt;-</span> loo_post<span class="sc">$</span>pointwise[,<span class="dv">1</span>]</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">paste</span>(<span class="st">&quot;ELPD (PSIS)=&quot;</span>,<span class="fu">round</span>(<span class="fu">sum</span>(elpd_psis),<span class="dv">2</span>)))</span></code></pre></div>
<pre><code>[1] &quot;ELPD (PSIS)= -42.51&quot;</code></pre>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">paste</span>(<span class="st">&quot;ELPD (MixIS)=&quot;</span>,<span class="fu">round</span>(<span class="fu">sum</span>(elpd_mixis),<span class="dv">2</span>)))</span></code></pre></div>
<pre><code>[1] &quot;ELPD (MixIS)= -44.94&quot;</code></pre>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">paste</span>(<span class="st">&quot;ELPD (brute force)=&quot;</span>,<span class="fu">round</span>(<span class="fu">sum</span>(elpd_loo),<span class="dv">2</span>)))</span></code></pre></div>
<pre><code>[1] &quot;ELPD (brute force)= -45.63&quot;</code></pre>
<p>In this example, MixIS provides a more accurate ELPD estimate closer
to the brute force estimate, while PSIS severely overestimates the ELPD.
Note that low accuracy of the PSIS ELPD estimate is expected in this
example given the large number of large Pareto-<span class="math inline">\(k\)</span> values. In this example, the accuracy
of MixIS estimate will also improve with bigger MCMC sample size.</p>
<p>More generally, mixture estimators can be useful in situations where
standard PSIS estimators struggle and return many large Pareto-<span class="math inline">\(k\)</span> values. In these contexts MixIS often
provides more accurate LOO-CV and ELPD estimates with a single sampling
routine (i.e. with a cost comparable to sampling from the original
posterior).</p>
</div>
<div id="references" class="section level2">
<h2>References</h2>
<p>Silva L. and Zanella G. (2022). Robust leave-one-out cross-validation
for high-dimensional Bayesian models. Preprint at <a href="https://arxiv.org/abs/2209.09190">arXiv:2209.09190</a></p>
<p>Vehtari A., Gelman A., and Gabry J. (2017). Practical Bayesian model
evaluation using leave-one-out cross-validation and WAIC. <em>Statistics
and Computing</em>, 27(5), 1413–1432. Preprint at <a href="https://arxiv.org/abs/1507.04544">arXiv:1507.04544</a></p>
<p>Vehtari A., Simpson D., Gelman A., Yao Y., and Gabry J. (2022).
Pareto smoothed importance sampling. Preprint at <a href="https://arxiv.org/abs/1507.02646">arXiv:1507.02646</a></p>
</div>
</div>



<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
